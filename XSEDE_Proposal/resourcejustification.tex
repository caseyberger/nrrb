\documentclass[onecolumn, 12pt]{article}
\usepackage{hyperref,mathtools,amssymb}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{floatrow}
\usepackage[margin=0.75in]{geometry}

\title{ERCAP Proposal: rotating bosons via CL with AMReX}
%\subject{Many-Body Quantum Mechanics}
\author{Casey Berger, Don Willcox}
\date{\today}

\newcommand{\casey}[1]{{\color{green} \bf[CB: #1]}}
\newcommand{\don}[1]{{\color{blue} \bf[DW: #1]}}
\newcommand{\etal}{{\it et al.}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\MarginPar}[1]{\hspace{1sp}\marginpar{\tiny\sffamily\raggedright\hspace{1sp}{\color{blue}#1}}}

\def\CP{{\mathcal P}}
\def\CC{{\mathcal C}}
\def\CH{{\mathcal H}}
\def\CW{{\mathcal W}}
\def\CO{{\mathcal O}}
\def\CZ{{\mathcal Z}}
\def\CD{{\mathcal D}}
\def\del{{\nabla}}

\newcommand*\dif{\mathop{}\!\mathrm{d}}

\begin{document}
\section{Project Description and Goals}
% TO DO
% \begin{itemize} 
% 	\item DONE  - can be shortened
	
% 	Goals: Please summarize the area of research on which you?ll be working during the 30-day Early User Program, and any specific research or performance goals that you hope to achieve.
	
% 	\item IN PROGRESS -- need to emphasize use of GPUs
	
% 	Resource selection: Please summarize why you have selected these specific resources (e.g., Regular Memory, Extreme Memory, and/or GPU nodes, and storage) to achieve these scientific goals.

%     {\color{blue} I've added some sentences to explain that we've put effort into using GPUs well and that has paid off with good performance that makes the Bridges-2 system ideal for this work.}
	
% 	\item Please confirm that your team agrees to provide feedback on the Early User Program, including a brief description of what you were able to achieve, near the conclusion of the 30-day Early User Program.

% 	{\color{blue} Added a sentence to the paragraph discussing performance.}
	
% 	\item Special considerations, if any (e.g., a need for VMs to support persistent web services or databases, interactive access to a large number of nodes, minimum of 2048 cores/job)
% \end{itemize}
% \subsection{Goals of the project}
This project aims to calculate properties of a rotating bosonic superfluid. Experimental study of superfluids under rotation have revealed the onset of quantized vortices with increasing rotational frequency, an example of macroscopic quantum behavior. Theoretical treatment of these systems struggles due to the presence of the ``sign problem," a challenge which plagues the study of many interesting quantum many-body systems. This project applies a method well-known in the community of lattice gauge theory, known as complex Langevin (CL), to circumvent the sign problem in the numerical study of a system of rotating, trapped, and interacting nonrelativistic bosons. 

Rotating bosonic systems can be described using the path-integral formulation:
%
\beq
\CZ = \int \dif \phi e^{-S[\phi]}
\eeq
%
with the action in $2+1$ dimensions given by
%
\beq
S = \int \dif x \dif y \dif\tau \left[ \phi^{*}\left( \partial_{\tau} - \frac{\del^{2}}{2m} - \mu  - \frac{m}{2} \omega_{\text{trap}}^{2}(x^{2}+y^{2})- i \omega_{z}(x \partial_{y} - y\partial_{x})\right)\phi + \lambda (\phi^{*} \phi)^{2}\right],
\eeq 
%
where our scalar fields are functions of space and euclidean time, $\phi(x,y,\tau)$. The bosonic fields are complex scalar fields, and the addition of a rotational term in the action makes the system irretrievably complex. In order to treat this action composed of complex-valued fields, we use the CL method.

Complex Langevin is a stochastic method for treating field-theoretical models with a complex action. It extends the well-established Langevin method to complex-valued fields, evolving those fields in a fictitious time called Langevin time, resulting in a set of field values distributed with the appropriate weight $e^{-S}$, from which quantities of interest can be computed via a simple statistical average. This treatment has been shown to be successful in toy models for finite quark density QCD~\cite{BergerCLReview} as well as in low energy atomic systems such as the polarized unitary Fermi gas~\cite{BergerCLReview}. 

I have developed and previously tested a code using CL to measure certain properties of this system. A sign problem emerges immediately in the formulation of the system with complex scalar fields, and CL has been shown to correctly reproduce known analytical solutions for simple versions of the system, i.e. with no rotation, external trap, or interaction. The code for this project is written in C++ and utilizes the AMReX framework. The adaptive mesh framework in AMReX will allow us to use a nonuniform mesh to calculate the field values, which enables more efficient use of computing resources. As the lattice size grows, we concentrate our resolution in the center of the lattice, where the interesting physics will be confined due to the presence of the harmonic traps, which reduce the presence of the fields rapidly to zero outside a narrow region in the center of the trap. Early work on this system with the additional features of rotation, trapping, and interaction have demonstrated the need to progress to significantly larger lattices in order to resolve the behavior of the rotating fluid on the lattice. Ultimately, we wish to produce calculations of quantized angular momentum and the density profiles of vortex formation in the fluid. 

To progress to the full system, it's essential to increase the computational resources in order to resolve much larger lattices. Scaling studies have been performed and tests have been done to elucidate the computational needs of this problem, and the results of these studies are discussed in Section~\ref{scaling}. We expect this work to result in multiple papers, as the application of this method to low-energy bosonic systems has been minimally studied in the past, and this work will result in insights into both the method and the system itself.

\section{Resource Selection and Computational Considerations}
% \subsection{\label{scaling}Scaling and Performance}
This code uses AMReX, a high-performance AMR library developed at LBNL and funded as an ECP Co-Design Center. We use AMReX data structures to discretize the spacetime lattice and distribute the lattice across MPI. We also use AMReX abstractions for performance portability to implement local parallelism with OpenMP or CUDA.
For each step in Langevin time, we update the spacetime lattice in a naively parallel fashion.
Because we divide the lattice into grids distributed across MPI ranks, we first fill ghost cells throughout the domain to support the nearest neighbor sums in the drift function.
We then calculate the drift function and Langevin update as entirely local operations for each MPI rank.

Our GPU memory approach utilizes AMReX local memory arenas allocated with CUDA Unified Memory, and we choose the number of GPUs for a given problem size to ensure all local grid data fits into GPU onboard memory to limit host-device transfers.
We have invested our development efforts in writing all our local work loops to fully exploit the data-parallel portions of the CL algorithm with CUDA kernels that calculate drift functions and the CL lattice update. We also leverage the CUDA random number generation library to accelerate calculating the stochastic terms for the lattice update.
%We parallelize this local work using OpenMP with logical tiling for CPUs or KNL to maintain cache efficiency. 
% When running on GPUs, we launch CUDA kernels for local work loops and disable logical tiling to ensure maximum kernel occupancy.
We efficiently calculate observables at runtime, which includes summing terms across the distributed lattice for density, field modulus, and circulation. On each MPI rank, we first calculate the observables on local grids in parallel using CUDA atomics and then combine these partial sums across the lattice with all-to-one MPI reductions.

% We developed and tested it on the NERSC Cori GPU development platform.
Our GPU strategy has paid off with significant speedups - we run fully $150\times$ faster with MPI+CUDA on 96 NVIDIA V100 GPUs compared with MPI+OpenMP on 96 Xeon Phi CPU-based accelerators.
In Fig.~\ref{Fig:GPUScaling} we also show excellent strong scaling on the Cori GPU test system using up to 96 GPUs for moderate and larger ($1024^3$) domains. Our development efforts and performance on GPUs make the Bridges-2 GPU nodes essential for efficiently running our proposed work, and we will share feedback on our achieved performance metrics and runtime configuration from this Early User Program with the Bridges-2 supercomputing team.

%We present scaling studies on Cori KNL in Fig.~\ref{Fig:KNLScaling} showing good strong scaling efficiency up to 10,000 KNL cores for a moderately sized domain of $512^3$ lattice sites.
% In Fig.~\ref{Fig:GPUScaling} we compare strong scaling curves on the Cori GPU development system for a $1024^3$ lattice for a run without observables (orange) and a run where we calculate observables every 10 Langevin steps (blue). We demonstrate that although the additional computation and communication for observables requires about twice as much walltime, our reduction strategy yields nearly ideal strong scaling efficiency.


%
%\vspace{-3mm}
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.6\textwidth]{./AMReX_Complex_Langevin_Cori_KNL_Scaling.png}
%\caption{\label{Fig:KNLScaling} Strong scaling for AMReX Complex Langevin on Knights Landing accelerators. These runs were performed with box sizes of $32^3$, 4 MPI tasks per node, and 16 OpenMP threads/MPI task.\vspace{-3mm}}
%\end{figure}
%\vspace{-3mm}
%

%
%\vspace{-3mm}
\begin{figure}[h]
\floatbox[{\capbeside\thisfloatsetup{capbesideposition={left,top},capbesidewidth=0.4\textwidth}}]{figure}[\FBwidth]
    {\caption{Strong scaling for AMReX Complex Langevin on NVIDIA Tesla V100 GPUs. These runs were performed with box sizes of $32^3$ and $128^3$ for the $512^3$ and $1024^3$ domains, respectively, using 1 MPI task per GPU and no logical tiling. For the run labeled ``O-10'' we calculate observables every 10 Langevin steps, otherwise we turn off observables to measure performance for the Langevin advance alone.}\label{Fig:GPUScaling}}
    {\includegraphics[width=0.6\textwidth]{./AMReX_Complex_Langevin_Cori_GPU_Scaling.png}}
\end{figure}
%\vspace{-3mm}
%

Initially, some further testing will need to be done on these larger lattices in order to determine the correct balance of trapping frequency, lattice size, rotation frequency, and interaction strengths. Once these parameter values are set, we will need to collect on the order of one hundred data points on each lattice (varying the chemical potential and the rotation frequency), where each data point is collected by performing a Langevin evolution of $10^{6}$ steps on a full $N_{x} \times N_{x} \times N_{\tau}$ lattice. We also wish to extend our time domain and examine the effect of decreasing the temperature on the formation of vortices, which will include a run at a fixed set of parameters for multiple lattices of varying temporal extent. %Ultimately, we expect to need around 30 Mhr to complete this study with rigor.

\bibliography{XSEDEbib}{}
%\bibliographystyle{ieeetr}
\bibliographystyle{plain}
\end{document}
